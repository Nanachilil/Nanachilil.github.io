<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>pytorch工作流 - Nanachilil的旧仓库</title><link rel="manifest" href="/manifest.json"><meta name="application-name" content="Nanachilil的旧仓库"><meta name="msapplication-TileImage" content="/img/logo_64.png"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Nanachilil的旧仓库"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="初学机器学习，从线性回归入手，了解机器学习的基本工作流程，从而进一步理解深度学习模型训练的本质。本博文主要用于整理个人的知识框架，希望也能帮到大家。如有不足，欢迎留言。🙏"><meta property="og:type" content="blog"><meta property="og:title" content="pytorch工作流"><meta property="og:url" content="https://nanachilil.com/posts/dfb3c4cb/"><meta property="og:site_name" content="Nanachilil的旧仓库"><meta property="og:description" content="初学机器学习，从线性回归入手，了解机器学习的基本工作流程，从而进一步理解深度学习模型训练的本质。本博文主要用于整理个人的知识框架，希望也能帮到大家。如有不足，欢迎留言。🙏"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://nanachilil.com/posts/dfb3c4cb/plot_predictions().png"><meta property="og:image" content="https://nanachilil.com/posts/dfb3c4cb/pytorch_%E5%B7%A5%E4%BD%9C%E6%B5%812.png"><meta property="og:image" content="https://nanachilil.com/posts/dfb3c4cb/pytorch_%E5%B7%A5%E4%BD%9C%E6%B5%813.png"><meta property="article:published_time" content="2024-01-16T08:13:04.000Z"><meta property="article:modified_time" content="2024-02-25T17:25:40.206Z"><meta property="article:author" content="Nanachilil"><meta property="article:tag" content="机器学习"><meta property="article:tag" content="Pytorch"><meta property="article:tag" content="线性回归"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://nanachilil.com/posts/dfb3c4cb/plot_predictions().png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://nanachilil.com/posts/dfb3c4cb/"},"headline":"pytorch工作流","image":["https://nanachilil.com/posts/dfb3c4cb/plot_predictions().png","https://nanachilil.com/posts/dfb3c4cb/pytorch_%E5%B7%A5%E4%BD%9C%E6%B5%812.png","https://nanachilil.com/posts/dfb3c4cb/pytorch_%E5%B7%A5%E4%BD%9C%E6%B5%813.png"],"datePublished":"2024-01-16T08:13:04.000Z","dateModified":"2024-02-25T17:25:40.206Z","author":{"@type":"Person","name":"Nanachilil"},"publisher":{"@type":"Organization","name":"Nanachilil的旧仓库","logo":{"@type":"ImageObject","url":"https://nanachilil.com/img/logo.png"}},"description":"初学机器学习，从线性回归入手，了解机器学习的基本工作流程，从而进一步理解深度学习模型训练的本质。本博文主要用于整理个人的知识框架，希望也能帮到大家。如有不足，欢迎留言。🙏"}</script><link rel="canonical" href="https://nanachilil.com/posts/dfb3c4cb/"><link rel="icon" href="/img/logo_64.png"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/font-awesome/6.0.0/css/all.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/highlight.js/11.7.0/styles/atom-one-light.min.css"><link rel="stylesheet"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/css/justifiedGallery.min.css"><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdnjs.loli.net/ajax/libs/pace/1.2.4/pace.min.js"></script><!--!--><!--!--><!-- hexo injector head_end start --><script>
  (function () {
      function switchTab() {
          if (!location.hash) {
            return;
          }

          const id = '#' + CSS.escape(location.hash.substring(1));
          const $tabMenu = document.querySelector(`.tabs a[href="${id}"]`);
          if (!$tabMenu) {
            return;
          }

          const $tabMenuContainer = $tabMenu.parentElement.parentElement;
          Array.from($tabMenuContainer.children).forEach($menu => $menu.classList.remove('is-active'));
          Array.from($tabMenuContainer.querySelectorAll('a'))
              .map($menu => document.getElementById($menu.getAttribute("href").substring(1)))
              .forEach($content => $content.classList.add('is-hidden'));

          if ($tabMenu) {
              $tabMenu.parentElement.classList.add('is-active');
          }
          const $activeTab = document.querySelector(id);
          if ($activeTab) {
              $activeTab.classList.remove('is-hidden');
          }
      }
      switchTab();
      window.addEventListener('hashchange', switchTab, false);
  })();
  </script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Nanachilil的旧仓库" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">时间轴</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/guestbook">留言板</a><a class="navbar-item" href="/about">关于</a></div><div class="navbar-end"><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3 is-size-4-mobile has-text-weight-normal"><i class="fas fa-angle-double-right"></i>pytorch工作流</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2024-01-16T08:13:04.000Z" title="2024-01-16T08:13:04.000Z">2024-01-16</time></span><span class="level-item"><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a><span> / </span><a class="link-muted" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Pytorch/">Pytorch</a></span><span class="level-item">12 分钟读完 (大约1858个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><div class="content"><html><head></head><body><p>初学机器学习，从线性回归入手，了解机器学习的基本工作流程，从而进一步理解深度学习模型训练的本质。本博文主要用于整理个人的知识框架，希望也能帮到大家。如有不足，欢迎留言。🙏</p>
<span id="more"></span>

<h1 id="Pytorch工作流"><a href="#Pytorch工作流" class="headerlink" title="Pytorch工作流"></a>Pytorch工作流</h1><p>运行环境：<a target="_blank" rel="noopener" href="https://colab.google/">https://colab.google/</a></p>
<p>将一般Pytorch机器学习的工作流程划分为：<code>数据准备</code>、<code>建立模型</code>、<code>训练模型</code>、<code>预测</code>这四个部分</p>
<h2 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h2><h3 id="准备数据集"><a href="#准备数据集" class="headerlink" title="准备数据集"></a>准备数据集</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment"># y = 0.7 * x + 0.3 (一系列散点)</span></span><br><span class="line">weight = <span class="number">0.7</span></span><br><span class="line">bias = <span class="number">0.3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create data</span></span><br><span class="line">start = <span class="number">0</span></span><br><span class="line">end = <span class="number">1</span></span><br><span class="line">step = <span class="number">0.02</span></span><br><span class="line">X = torch.arange(start,end, step)</span><br><span class="line">y = weight * X +bias</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">X <span class="comment"># --&gt;表示单步运行结果</span></span><br><span class="line">--&gt;tensor([<span class="number">0.0000</span>, <span class="number">0.0200</span>, <span class="number">0.0400</span>, <span class="number">0.0600</span>, <span class="number">0.0800</span>, <span class="number">0.1000</span>, <span class="number">0.1200</span>, <span class="number">0.1400</span>, <span class="number">0.1600</span>,</span><br><span class="line">        <span class="number">0.1800</span>, <span class="number">0.2000</span>, <span class="number">0.2200</span>, <span class="number">0.2400</span>, <span class="number">0.2600</span>, <span class="number">0.2800</span>, <span class="number">0.3000</span>, <span class="number">0.3200</span>, <span class="number">0.3400</span>,</span><br><span class="line">        <span class="number">0.3600</span>, <span class="number">0.3800</span>, <span class="number">0.4000</span>, <span class="number">0.4200</span>, <span class="number">0.4400</span>, <span class="number">0.4600</span>, <span class="number">0.4800</span>, <span class="number">0.5000</span>, <span class="number">0.5200</span>,</span><br><span class="line">        <span class="number">0.5400</span>, <span class="number">0.5600</span>, <span class="number">0.5800</span>, <span class="number">0.6000</span>, <span class="number">0.6200</span>, <span class="number">0.6400</span>, <span class="number">0.6600</span>, <span class="number">0.6800</span>, <span class="number">0.7000</span>,</span><br><span class="line">        <span class="number">0.7200</span>, <span class="number">0.7400</span>, <span class="number">0.7600</span>, <span class="number">0.7800</span>, <span class="number">0.8000</span>, <span class="number">0.8200</span>, <span class="number">0.8400</span>, <span class="number">0.8600</span>, <span class="number">0.8800</span>,</span><br><span class="line">        <span class="number">0.9000</span>, <span class="number">0.9200</span>, <span class="number">0.9400</span>, <span class="number">0.9600</span>, <span class="number">0.9800</span>])</span><br><span class="line">        </span><br><span class="line">y</span><br><span class="line">--&gt;tensor([<span class="number">0.3000</span>, <span class="number">0.3140</span>, <span class="number">0.3280</span>, <span class="number">0.3420</span>, <span class="number">0.3560</span>, <span class="number">0.3700</span>, <span class="number">0.3840</span>, <span class="number">0.3980</span>, <span class="number">0.4120</span>,</span><br><span class="line">        <span class="number">0.4260</span>, <span class="number">0.4400</span>, <span class="number">0.4540</span>, <span class="number">0.4680</span>, <span class="number">0.4820</span>, <span class="number">0.4960</span>, <span class="number">0.5100</span>, <span class="number">0.5240</span>, <span class="number">0.5380</span>,</span><br><span class="line">        <span class="number">0.5520</span>, <span class="number">0.5660</span>, <span class="number">0.5800</span>, <span class="number">0.5940</span>, <span class="number">0.6080</span>, <span class="number">0.6220</span>, <span class="number">0.6360</span>, <span class="number">0.6500</span>, <span class="number">0.6640</span>,</span><br><span class="line">        <span class="number">0.6780</span>, <span class="number">0.6920</span>, <span class="number">0.7060</span>, <span class="number">0.7200</span>, <span class="number">0.7340</span>, <span class="number">0.7480</span>, <span class="number">0.7620</span>, <span class="number">0.7760</span>, <span class="number">0.7900</span>,</span><br><span class="line">        <span class="number">0.8040</span>, <span class="number">0.8180</span>, <span class="number">0.8320</span>, <span class="number">0.8460</span>, <span class="number">0.8600</span>, <span class="number">0.8740</span>, <span class="number">0.8880</span>, <span class="number">0.9020</span>, <span class="number">0.9160</span>,</span><br><span class="line">        <span class="number">0.9300</span>, <span class="number">0.9440</span>, <span class="number">0.9580</span>, <span class="number">0.9720</span>, <span class="number">0.9860</span>])</span><br></pre></td></tr></tbody></table></figure>



<h3 id="划分数据集"><a href="#划分数据集" class="headerlink" title="划分数据集"></a>划分数据集</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train/test split</span></span><br><span class="line">train_split = <span class="built_in">int</span>(<span class="number">0.8</span> * <span class="built_in">len</span>(X)) <span class="comment"># 8:2的划分</span></span><br><span class="line">X_train, y_train = X[:train_split], y[:train_split] <span class="comment">#左闭右开</span></span><br><span class="line">X_test, y_test = X[train_split:], y[train_split:]</span><br><span class="line"></span><br><span class="line"><span class="built_in">len</span>(X_train), <span class="built_in">len</span>(y_train), <span class="built_in">len</span>(X_test), <span class="built_in">len</span>(y_test)</span><br><span class="line">--&gt;(<span class="number">40</span>, <span class="number">40</span>, <span class="number">10</span>, <span class="number">10</span>)</span><br></pre></td></tr></tbody></table></figure>



<h3 id="可视化"><a href="#可视化" class="headerlink" title="可视化"></a>可视化</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt <span class="comment">#import as 为引入模块并起别名</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">plot_predictions</span>(<span class="params">train_data = X_train, train_labels = y_train, test_data = X_test, test_labels = y_test, predictions = <span class="literal">None</span></span>):</span><br><span class="line">  plt.figure(figsize = (<span class="number">10</span>, <span class="number">7</span>)) <span class="comment"># 图像大小</span></span><br><span class="line">  plt.scatter(train_data, train_labels, c = <span class="string">"b"</span>, s = <span class="number">4</span>, label = <span class="string">"Training data"</span>) <span class="comment"># s = 4, 为指定散点的直径，大约为4个点</span></span><br><span class="line">  plt.scatter(test_data, test_labels, c = <span class="string">"g"</span>, s = <span class="number">4</span>, label = <span class="string">"Test Data"</span>)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> predictions <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    plt.scatter(test_data, predictions, c= <span class="string">"r"</span>, s = <span class="number">4</span>, label = <span class="string">"Prediction"</span>)</span><br><span class="line">  plt.legend(prop = {<span class="string">"size"</span>:<span class="number">14</span>}) <span class="comment"># 图例的属性，其中 size 参数指定了图例中文本的大小为 14 磅</span></span><br><span class="line"></span><br><span class="line">plot_predictions()</span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plot_predictions()</span><br><span class="line">--&gt;</span><br></pre></td></tr></tbody></table></figure>

<p><img src="/posts/dfb3c4cb/plot_predictions().png" alt="download"></p>
<h2 id="建立模型"><a href="#建立模型" class="headerlink" title="建立模型"></a>建立模型</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearRegressionModel</span>(nn.Module):  <span class="comment">#这个类继承自 nn.Module 类，这是 PyTorch 中定义神经网络模型的基类。</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="built_in">super</span>().__init__()</span><br><span class="line">    <span class="comment"># nn.Parameter 是 PyTorch 中的一个类，用于将张量（tensor）封装为可学习的模型参数,</span></span><br><span class="line">    <span class="comment"># 这些参数将被自动地添加到模型的参数列表中，并能够被优化器识别和更新,并使得其具有梯度计算和自动求导的功能。</span></span><br><span class="line">    <span class="comment"># 在神经网络中，模型参数通常表示为权重和偏置，它们需要在训练过程中通过梯度下降等优化算法进行更新。</span></span><br><span class="line">    self.weights = nn.Parameter(torch.randn(<span class="number">1</span>, dtype = torch.<span class="built_in">float</span>), requires_grad = <span class="literal">True</span>) 		 <span class="comment">#requires_grad = true 表示用于梯度计算,以便在训练过程中更新它们的值。</span></span><br><span class="line">    self.bias = nn.Parameter(torch.randn(<span class="number">1</span>,dtype = torch.<span class="built_in">float</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 我们定义了两个参数：权重（weights）和偏置（bias），它们都是可学习的模型参数，用于线性回归模型中的线性部分。</span></span><br><span class="line">    <span class="comment"># 这些参数通过 nn.Parameter 函数封装为模型参数，从而使得它们可以被优化器更新。</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x:torch.tensor</span>):   <span class="comment">#x:torch.tensor标识注释(免责声明)， 不强制，也可以接受其他类型， -&gt;torch.tensor也表示注释，意为最后得到的数据类型</span></span><br><span class="line">    <span class="keyword">return</span> self.weights * x + self.bias</span><br></pre></td></tr></tbody></table></figure>



<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">torch.manual_seed(<span class="number">666</span>) <span class="comment"># 做个种子，保证伪随机的一致性</span></span><br><span class="line">model_0 = LinearRegressionModel() <span class="comment"># model_0 赋为一个具有相应类属性的对象</span></span><br><span class="line"><span class="comment"># 注意model_0没有在训练模型处声明，所以每次调参训练结果会追加。所以在google colab中每次调参重新训练，应该从这里开始。</span></span><br><span class="line"></span><br><span class="line">model_0.parameters()</span><br><span class="line"><span class="comment"># model_0.parameters() 用于获取神经网络模型 model_0 中的所有模型参数。</span></span><br><span class="line"><span class="comment"># 在 PyTorch 中，模型参数通常是由 nn.Parameter 类型的对象表示的，它们存储了模型的权重和偏置等可学习的参数。</span></span><br><span class="line"><span class="comment"># model_0.parameters() 返回一个迭代器，通过迭代器可以依次访问模型中的每一个参数。这个迭代器通常用于将模型的参数传递给优化器，以便在训练过程中更新参数。</span></span><br><span class="line">--&gt;&lt;generator <span class="built_in">object</span> Module.parameters at <span class="number">0x7a74a42ae7a0</span>&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果想print一个东西，但是打印出类似于&lt;generator object Module.parameters at 0x79fc67df00b0&gt;</span></span><br><span class="line"><span class="comment"># 则强制类型转换为list</span></span><br><span class="line"><span class="built_in">list</span>(model_0.parameters())</span><br><span class="line">--&gt;[Parameter containing:</span><br><span class="line"> tensor([-<span class="number">2.1188</span>], requires_grad=<span class="literal">True</span>),</span><br><span class="line"> Parameter containing:</span><br><span class="line"> tensor([<span class="number">0.0635</span>], requires_grad=<span class="literal">True</span>)]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model_0.state_dict() <span class="comment"># 另一种得到他的参数的方式 用于返回模型的参数字典（state dictionary）。这个字典包含了模型中所有可学习参数的名称及其对应的张量值。</span></span><br><span class="line">--&gt;OrderedDict([(<span class="string">'weights'</span>, tensor([-<span class="number">2.1188</span>])), (<span class="string">'bias'</span>, tensor([<span class="number">0.0635</span>]))])</span><br></pre></td></tr></tbody></table></figure>



<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### 训练一个模型</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### loss function ，损失函数值，越大，数据越乱。用于度量y_pred和y_test之间的差距</span></span><br><span class="line"><span class="comment"># MAE 3 4 5; 2 6 4 = 1.33</span></span><br><span class="line"><span class="comment"># MSE 3 4 5; 2 6 4 = 2</span></span><br><span class="line"><span class="comment"># 两种比较方式，每种参数只和自己比</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### optimizer ，告诉我们的模型怎样朝向正确的参数走过去</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">由于google colab是单步运行，所以每次调参后，应该从建立model_0处重新向后运行</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.L1Loss() <span class="comment">#MAE,</span></span><br><span class="line"><span class="comment"># nn.L1Loss() 是 PyTorch 中的一个损失函数，用于计算预测值和目标值之间的平均绝对误差（Mean Absolute Error，MAE）。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.optim.SGD: 这是 PyTorch 中的随机梯度下降优化器的类。它实现了随机梯度下降算法，用于更新模型的参数以最小化损失函数。这个优化器需要指定要优化的参数和学习率等超参数。</span></span><br><span class="line">optimizer = torch.optim.SGD(params = model_0.parameters(), lr = <span class="number">0.1</span>) <span class="comment">#lr 怎么调（跑了一个模型后观察一下，曲线平，则加大10倍），一般0.01、0.1没问题</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">epochs = <span class="number">100</span></span><br><span class="line"></span><br><span class="line">train_loss_values = []</span><br><span class="line">test_loss_values = []</span><br><span class="line">epoch_count = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Put model in training mode:</span></span><br><span class="line">  model_0.train()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 1. forward pass, using forward function</span></span><br><span class="line">  y_pred = model_0(X_train)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 2. Calculate loss</span></span><br><span class="line">  loss = loss_fn(y_pred, y_train)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 3. zero_grad 在每一次迭代中，我们都需要在反向传播之前将梯度归零，</span></span><br><span class="line">  <span class="comment">#这是因为 PyTorch 默认会在进行梯度计算时，将新计算得到的梯度累加到已存在的梯度上，</span></span><br><span class="line">  <span class="comment">#而我们希望每次迭代都是独立的。因此，使用 optimizer.zero_grad() 就可以很方便地实现这一步骤。</span></span><br><span class="line">  optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 4. back prop 梯度计算</span></span><br><span class="line">  loss.backward()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 5. Progress optimizer 更新梯度</span></span><br><span class="line">  optimizer.step()</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 6. 调用 model_0.eval() 将模型切换到评估模式（不会梯度计算）。</span></span><br><span class="line">  model_0.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">  <span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    test_pred = model_0(X_test)</span><br><span class="line">    epoch_count.append(epoch)</span><br><span class="line">    test_loss = loss_fn(test_pred, y_test.<span class="built_in">type</span>(torch.<span class="built_in">float</span>))</span><br><span class="line">    train_loss_values.append(loss.detach().numpy())  <span class="comment"># loss.detach().numpy() 将 PyTorch 张量转换为 NumPy 数组时，通常是为了利用 NumPy 提供的丰富功能进行数据处理、分析或者可视化。</span></span><br><span class="line">    test_loss_values.append(test_loss.detach().numpy())</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch:<span class="subst">{epoch}</span> | Train Loss:<span class="subst">{loss}</span> | Test Loss:<span class="subst">{test_loss}</span>"</span>)</span><br><span class="line">    </span><br><span class="line">--&gt;Epoch:<span class="number">0</span> | Train Loss:<span class="number">1.3358044624328613</span> | Test Loss:<span class="number">2.610489845275879</span></span><br><span class="line">Epoch:<span class="number">1</span> | Train Loss:<span class="number">1.2205944061279297</span> | Test Loss:<span class="number">2.4757800102233887</span></span><br><span class="line">Epoch:<span class="number">2</span> | Train Loss:<span class="number">1.1053844690322876</span> | Test Loss:<span class="number">2.3410699367523193</span></span><br><span class="line">Epoch:<span class="number">3</span> | Train Loss:<span class="number">0.9938250780105591</span> | Test Loss:<span class="number">2.216449022293091</span></span><br><span class="line">Epoch:<span class="number">4</span> | Train Loss:<span class="number">0.9000433683395386</span> | Test Loss:<span class="number">2.097005844116211</span></span><br><span class="line">Epoch:<span class="number">5</span> | Train Loss:<span class="number">0.8182994723320007</span> | Test Loss:<span class="number">1.9881856441497803</span></span><br><span class="line">Epoch:<span class="number">6</span> | Train Loss:<span class="number">0.7505138516426086</span> | Test Loss:<span class="number">1.8903448581695557</span></span><br><span class="line">......</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 绘图</span></span><br><span class="line">plt.plot(epoch_count, train_loss_values, label = <span class="string">"Train Loss"</span>)</span><br><span class="line">plt.plot(epoch_count, test_loss_values, label = <span class="string">"Test Loss"</span>)</span><br><span class="line"></span><br><span class="line">plt.title(<span class="string">"Training and test loss curves"</span>)</span><br><span class="line">plt.ylabel(<span class="string">"Loss"</span>)</span><br><span class="line">plt.xlabel(<span class="string">"Epochs"</span>)</span><br><span class="line"></span><br><span class="line">plt.legend()</span><br></pre></td></tr></tbody></table></figure>

<p><img src="/posts/dfb3c4cb/pytorch_%E5%B7%A5%E4%BD%9C%E6%B5%812.png" alt="pytorch_工作流2"></p>
<h2 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">model_0.<span class="built_in">eval</span>() <span class="comment">#评价模式</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.inference_mode(): <span class="comment">#做预测</span></span><br><span class="line">  y_preds = model_0(X_test)</span><br><span class="line"></span><br><span class="line">y_preds</span><br><span class="line">--&gt;tensor([<span class="number">0.5704</span>, <span class="number">0.5693</span>, <span class="number">0.5683</span>, <span class="number">0.5672</span>, <span class="number">0.5661</span>, <span class="number">0.5650</span>, <span class="number">0.5639</span>, <span class="number">0.5629</span>, <span class="number">0.5618</span>,</span><br><span class="line">        <span class="number">0.5607</span>])</span><br><span class="line">model_0.state_dict()</span><br><span class="line">--&gt;OrderedDict([(<span class="string">'weights'</span>, tensor([-<span class="number">0.0539</span>])), (<span class="string">'bias'</span>, tensor([<span class="number">0.6135</span>]))])</span><br><span class="line"></span><br><span class="line">plot_predictions(predictions = y_preds)</span><br></pre></td></tr></tbody></table></figure>

<p><img src="/posts/dfb3c4cb/pytorch_%E5%B7%A5%E4%BD%9C%E6%B5%813.png" alt="pytorch_工作流3"></p>
<h3 id="保存模型与数据读取"><a href="#保存模型与数据读取" class="headerlink" title="保存模型与数据读取"></a>保存模型与数据读取</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 保存模型到google 云端硬盘</span></span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line">MODEL_PATH = Path(<span class="string">"models"</span>)</span><br><span class="line"></span><br><span class="line">MODEL_PATH.mkdir(parents = <span class="literal">True</span>, exist_ok = <span class="literal">True</span>)</span><br><span class="line">MODEL_NAME = <span class="string">"pytorch_workflow_model_0.pth"</span></span><br><span class="line">MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME</span><br><span class="line">MODEL_SAVE_PATH</span><br><span class="line"></span><br><span class="line">torch.save(obj = model_0.state_dict(), f = MODEL_SAVE_PATH)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 读取数据至新模型</span></span><br><span class="line">loaded_model_0 = LinearRegressionModel()</span><br><span class="line">loaded_model_0.state_dict()</span><br><span class="line"><span class="comment"># 赋值参数</span></span><br><span class="line">loaded_model_0.load_state_dict(torch.load(f = MODEL_SAVE_PATH))</span><br><span class="line"></span><br><span class="line">                         </span><br></pre></td></tr></tbody></table></figure>



</body></html></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习, </a><a class="link-muted" rel="tag" href="/tags/Pytorch/">Pytorch, </a><a class="link-muted" rel="tag" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/">线性回归 </a></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/zhifubao.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechat.png" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/posts/e8eb0481/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">二分查找</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/posts/1a827395/"><span class="level-item">评论区食用说明</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="content twikoo" id="twikoo"></div><script src="https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js"></script><script>twikoo.init({
      envId: 'https://twikoo-pi-five.vercel.app/'
    });</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/bin.jpg" alt="Nanachilil"></figure><p class="title is-size-4 is-block" style="line-height:inherit;">Nanachilil</p><p class="is-size-6 is-block">Nanachilil的旧仓库</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>WuXi, China</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">17</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">12</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">24</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/Nanachilil" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://space.bilibili.com/473691769"><i class="fa-brands fa-bilibili"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="mail" href="mailto:last1850@outlook.com"><i class="fas fa-envelope"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="QQ" href="http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&amp;k=USadAmK9UBNWzBfkIB0VQIjJLr1tRkpM&amp;authKey=xVKjTKf%2FE9NQGBgPRM27qMq7JJ2HpbXvLX0m3AZUvH9naIfO4qpZbcojsKrN%2BYH4&amp;noverify=0&amp;group_code=876827316"><i class="fab fa-qq"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Pytorch工作流"><span class="level-left"><span class="level-item">1</span><span class="level-item">Pytorch工作流</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#数据准备"><span class="level-left"><span class="level-item">1.1</span><span class="level-item">数据准备</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#准备数据集"><span class="level-left"><span class="level-item">1.1.1</span><span class="level-item">准备数据集</span></span></a></li><li><a class="level is-mobile" href="#划分数据集"><span class="level-left"><span class="level-item">1.1.2</span><span class="level-item">划分数据集</span></span></a></li><li><a class="level is-mobile" href="#可视化"><span class="level-left"><span class="level-item">1.1.3</span><span class="level-item">可视化</span></span></a></li></ul></li><li><a class="level is-mobile" href="#建立模型"><span class="level-left"><span class="level-item">1.2</span><span class="level-item">建立模型</span></span></a></li><li><a class="level is-mobile" href="#训练模型"><span class="level-left"><span class="level-item">1.3</span><span class="level-item">训练模型</span></span></a></li><li><a class="level is-mobile" href="#预测"><span class="level-left"><span class="level-item">1.4</span><span class="level-item">预测</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#保存模型与数据读取"><span class="level-left"><span class="level-item">1.4.1</span><span class="level-item">保存模型与数据读取</span></span></a></li></ul></li></ul></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget" data-type="categories"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile" href="/categories/hexo/"><span class="level-start"><span class="level-item">hexo</span></span><span class="level-end"><span class="level-item tag">5</span></span></a><ul><li><a class="level is-mobile" href="/categories/hexo/icarus/"><span class="level-start"><span class="level-item">icarus</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile" href="/categories/hexo/twikoo/"><span class="level-start"><span class="level-item">twikoo</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/hexo/%E6%8A%A5%E9%94%99/"><span class="level-start"><span class="level-item">报错</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="level-start"><span class="level-item">机器学习</span></span><span class="level-end"><span class="level-item tag">3</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Pytorch/"><span class="level-start"><span class="level-item">Pytorch</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/Pytorch-%E5%A4%9A%E5%88%86%E7%B1%BB/"><span class="level-start"><span class="level-item">Pytorch,多分类</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/"><span class="level-start"><span class="level-item">算法</span></span><span class="level-end"><span class="level-item tag">9</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/%E4%BA%8C%E5%88%86%E6%9F%A5%E6%89%BE/"><span class="level-start"><span class="level-item">二分查找</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/%E5%8D%95%E9%93%BE%E8%A1%A8/"><span class="level-start"><span class="level-item">单链表</span></span><span class="level-end"><span class="level-item tag">7</span></span></a><ul><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/%E5%8D%95%E9%93%BE%E8%A1%A8/%E6%A0%88/"><span class="level-start"><span class="level-item">栈</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li><li><a class="level is-mobile" href="/categories/%E7%AE%97%E6%B3%95/%E6%A0%88/"><span class="level-start"><span class="level-item">栈</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></li></ul></div></div></div><div class="card widget" data-type="tags"><div class="card-content"><div class="menu"><h3 class="menu-label">标签</h3><div class="field is-grouped is-grouped-multiline"><div class="control"><a class="tags has-addons" href="/tags/Pytorch/"><span class="tag">Pytorch</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/Spawn-failed/"><span class="tag">Spawn_failed</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/hexo/"><span class="tag">hexo</span><span class="tag">4</span></a></div><div class="control"><a class="tags has-addons" href="/tags/icarus/"><span class="tag">icarus</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/twikoo/"><span class="tag">twikoo</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E4%B8%8D%E5%B8%A6%E5%A4%B4%E7%BB%93%E7%82%B9/"><span class="tag">不带头结点</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%83%E7%B4%A0%E9%80%92%E5%A2%9E/"><span class="tag">元素递增</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%85%B1%E4%BA%AB%E6%A0%88/"><span class="tag">共享栈</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1/"><span class="tag">分类任务</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%88%A4%E6%96%AD%E5%AF%B9%E7%A7%B0%E6%80%A7/"><span class="tag">判断对称性</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%8D%95%E9%93%BE%E8%A1%A8/"><span class="tag">单链表</span><span class="tag">7</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E5%B8%A6%E5%A4%B4%E7%BB%93%E7%82%B9/"><span class="tag">带头结点</span><span class="tag">6</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8C%89%E5%80%BC%E5%88%A0%E9%99%A4/"><span class="tag">按值删除</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%8F%92%E5%85%A5%E5%9B%BE%E7%89%87%E5%8A%9F%E8%83%BD/"><span class="tag">插入图片功能</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%97%A5%E5%BF%97/"><span class="tag">日志</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%80%E5%B0%8F%E5%80%BC/"><span class="tag">最小值</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"><span class="tag">机器学习</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%9F%A5%E6%89%BE/"><span class="tag">查找</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E6%A0%88/"><span class="tag">栈</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%A9%BA%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E4%BD%8E/"><span class="tag">空间复杂度低</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"><span class="tag">线性回归</span><span class="tag">1</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%86%E5%BA%8F/"><span class="tag">逆序</span><span class="tag">2</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%80%92%E5%BD%92/"><span class="tag">递归</span><span class="tag">3</span></a></div><div class="control"><a class="tags has-addons" href="/tags/%E9%AB%98%E6%95%88%E7%AE%97%E6%B3%95/"><span class="tag">高效算法</span><span class="tag">1</span></a></div></div></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Nanachilil的旧仓库" height="28"></a><p class="is-size-7"><span>&copy; 2024 Nanachilil</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p><p class="is-size-7">© 2019</p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/ppoffice/hexo-theme-icarus"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdnjs.loli.net/ajax/libs/jquery/3.3.1/jquery.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/moment.js/2.22.2/moment-with-locales.min.js"></script><script src="https://cdnjs.loli.net/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" defer></script><script>moment.locale("zh-cn");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdnjs.loli.net/ajax/libs/cookieconsent/3.1.1/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-right",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdnjs.loli.net/ajax/libs/lightgallery/1.10.0/js/lightgallery.min.js" defer></script><script src="https://cdnjs.loli.net/ajax/libs/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/custom.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body></html>